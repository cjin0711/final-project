{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1400, 17)\n",
      "Dataset columns: ['user_media_count', 'user_follower_count', 'user_following_count', 'user_has_highligh_reels', 'user_has_external_url', 'user_tags_count', 'follower_following_ratio', 'user_biography_length', 'username_length', 'username_digit_count', 'media_comment_numbers', 'media_comments_are_disabled', 'media_has_location_info', 'media_hashtag_numbers', 'media_like_numbers', 'mediaUpload_times', 'automated_behaviour']\n",
      "List columns that need preprocessing: ['media_comment_numbers', 'media_comments_are_disabled', 'media_has_location_info', 'media_hashtag_numbers', 'media_like_numbers', 'mediaUpload_times']\n",
      "1. user_media_count\n",
      "2. user_follower_count\n",
      "3. user_following_count\n",
      "4. user_has_highligh_reels\n",
      "5. user_has_external_url\n",
      "6. user_tags_count\n",
      "7. follower_following_ratio\n",
      "8. user_biography_length\n",
      "9. username_length\n",
      "10. username_digit_count\n",
      "11. media_comment_numbers\n",
      "12. media_comments_are_disabled\n",
      "13. media_has_location_info\n",
      "14. media_hashtag_numbers\n",
      "15. media_like_numbers\n",
      "16. mediaUpload_times\n",
      "\n",
      "Total number of features: 16\n",
      "(array([0, 1]), array([420, 420]))\n",
      "Processed features shape: (1400, 16)\n",
      "Training features shape: (840, 16)\n",
      "Validation features shape: (280, 16)\n",
      "Testing features shape: (280, 16)\n",
      "Scaled training data shape: (840, 16)\n",
      "\n",
      "Columns in X_processed:\n",
      "1. user_media_count\n",
      "2. user_follower_count\n",
      "3. user_following_count\n",
      "4. user_has_highligh_reels\n",
      "5. user_has_external_url\n",
      "6. user_tags_count\n",
      "7. follower_following_ratio\n",
      "8. user_biography_length\n",
      "9. username_length\n",
      "10. media_comment_numbers\n",
      "11. media_has_location_info\n",
      "12. media_hashtag_numbers\n",
      "13. media_like_numbers\n",
      "14. mediaUpload_times_interval_std\n",
      "\n",
      "Total number of features: 14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, QuantileTransformer, StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To allow for importing of 'utils' module from parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils import import_data\n",
    "\n",
    "dataset_path = \"../data\"\n",
    "dataset_version = \"automated-v1.0\"\n",
    "\n",
    "# Import the dataset (returns a dictionary)\n",
    "automated_dataset = import_data(dataset_path, dataset_version)\n",
    "\n",
    "# Extract the dataframe from the dictionary\n",
    "df = automated_dataset['dataframe']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "df.head()\n",
    "\n",
    "def extract_timestamp_features(timestamps):\n",
    "    # Sort timestamps\n",
    "    sorted_timestamps = sorted(timestamps)\n",
    "    \n",
    "    # Calculate intervals between posts\n",
    "    intervals = [sorted_timestamps[i+1] - sorted_timestamps[i] \n",
    "                for i in range(len(sorted_timestamps)-1)]\n",
    "    \n",
    "    # Key features\n",
    "    features = {\n",
    "        \"interval_std\": np.std(intervals),\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Split into features and target\n",
    "X = df.drop('automated_behaviour', axis=1)  \n",
    "y = df['automated_behaviour']  \n",
    "\n",
    "# Check for columns containing lists\n",
    "list_columns = []\n",
    "for col in X.columns:\n",
    "    if isinstance(X[col].iloc[0], list):\n",
    "        list_columns.append(col)\n",
    "\n",
    "print(f\"List columns that need preprocessing: {list_columns}\")\n",
    "\n",
    "# Process list columns to extract numeric features\n",
    "X_processed = X.copy()\n",
    "\n",
    "# Define binary and numeric columns\n",
    "binary_columns = ['media_comments_are_disabled', 'media_has_location_info']\n",
    "numeric_columns = ['media_comment_numbers', 'media_hashtag_numbers', 'media_like_numbers']\n",
    "\n",
    "for i, col in enumerate(X_processed.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "print(f\"\\nTotal number of features: {len(X_processed.columns)}\")\n",
    "\n",
    "for col in list_columns:\n",
    "    if col == 'mediaUpload_times':  # Special handling for timestamps\n",
    "        # Extract timestamp features\n",
    "        X_processed[f'{col}_interval_std'] = X_processed[col].apply(\n",
    "            lambda x: extract_timestamp_features(x)['interval_std'] if len(x) > 1 else 0\n",
    "        )\n",
    "        X_processed = X_processed.drop(col, axis=1)\n",
    "    elif col in binary_columns:\n",
    "        # For binary features, sum up the 1s\n",
    "        X_processed[col] = X_processed[col].apply(np.sum)\n",
    "    elif col in numeric_columns:\n",
    "        # For numeric features, take the average\n",
    "        X_processed[col] = X_processed[col].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Splitting the dataset (20% Testing, 20% Validating, 60% Training)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_processed, y,   # Changed X to X_processed here\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, \n",
    "    test_size=0.25, \n",
    "    random_state=42, \n",
    "    shuffle=True, \n",
    "    stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "\n",
    "# Apply StandardScaler to the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)    # Added validation set scaling\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Processed features shape: {X_processed.shape}\")\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Validation features shape: {X_val.shape}\")    # Added validation shape\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "\n",
    "columns_to_drop = ['media_comments_are_disabled', 'username_digit_count']\n",
    "X_processed = X_processed.drop(columns=columns_to_drop)\n",
    "print(\"\\nColumns in X_processed:\")\n",
    "for i, col in enumerate(X_processed.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "print(f\"\\nTotal number of features: {len(X_processed.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (840). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "c_values = [0.001, 0.01, 0.1, 1, 10, 100]  # We'll use these as alpha=1/C for regularization\n",
    "\n",
    "# Prepare transformed datasets (reuse your previous code)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_val_poly = poly.transform(X_val_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "X_train_log = np.log1p(X_train_scaled - X_train_scaled.min(axis=0) + 1)\n",
    "X_val_log = np.log1p(X_val_scaled - X_train_scaled.min(axis=0) + 1)\n",
    "X_test_log = np.log1p(X_test_scaled - X_train_scaled.min(axis=0) + 1)\n",
    "\n",
    "quantile = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "X_train_quant = quantile.fit_transform(X_train_scaled)\n",
    "X_val_quant = quantile.transform(X_val_scaled)\n",
    "X_test_quant = quantile.transform(X_test_scaled)\n",
    "\n",
    "transformations = {\n",
    "    'Polynomial': (X_train_poly, X_val_poly, X_test_poly),\n",
    "    'Logarithmic': (X_train_log, X_val_log, X_test_log),\n",
    "    'Quantile (Normal)': (X_train_quant, X_val_quant, X_test_quant)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best for Polynomial:\n",
      "  C=1, Val Accuracy=0.9214, Test Accuracy=0.9393\n",
      "  Final Validation Precision: 0.9609\n",
      "  Final Validation Recall:    0.8786\n",
      "\n",
      "Best for Logarithmic:\n",
      "  C=10, Val Accuracy=0.9250, Test Accuracy=0.9357\n",
      "  Final Validation Precision: 0.9685\n",
      "  Final Validation Recall:    0.8786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best for Quantile (Normal):\n",
      "  C=1, Val Accuracy=0.9464, Test Accuracy=0.9179\n",
      "  Final Validation Precision: 0.9845\n",
      "  Final Validation Recall:    0.9071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suisc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_val_preds = {}\n",
    "all_test_preds = {}\n",
    "\n",
    "for trans_name, (Xtr, Xv, Xte) in transformations.items():\n",
    "    best_val_acc = -np.inf\n",
    "    best_c = None\n",
    "    best_val_prec = None\n",
    "    best_val_rec = None\n",
    "    best_test_acc = None\n",
    "    for c in c_values:\n",
    "        alpha = 1.0 / c  # Regularization parameter for MLPClassifier\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=(64, 32),  # You can adjust the architecture\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=alpha,\n",
    "            max_iter=300,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(Xtr, y_train)\n",
    "        # Training metrics\n",
    "        y_train_pred = model.predict(Xtr)\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        # Validation metrics\n",
    "        y_val_pred = model.predict(Xv)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        # Test metrics\n",
    "        y_test_pred = model.predict(Xte)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        all_results.append({\n",
    "            'Transformation': trans_name,\n",
    "            'C': c,\n",
    "            'Train Accuracy': train_acc,\n",
    "            'Val Accuracy': val_acc,\n",
    "            'Test Accuracy': test_acc\n",
    "        })\n",
    "        all_val_preds[(trans_name, c)] = y_val_pred\n",
    "        all_test_preds[(trans_name, c)] = y_test_pred\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_c = c\n",
    "            best_val_prec = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "            best_val_rec = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "            best_test_acc = test_acc\n",
    "    print(f\"\\nBest for {trans_name}:\")\n",
    "    print(f\"  C={best_c}, Val Accuracy={best_val_acc:.4f}, Test Accuracy={best_test_acc:.4f}\")\n",
    "    print(f\"  Final Validation Precision: {best_val_prec:.4f}\")\n",
    "    print(f\"  Final Validation Recall:    {best_val_rec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
